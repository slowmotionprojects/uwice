{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import datetime\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [('../data/raw/Baen4185_ODBA3D_c.csv', '../data/raw/Baen4185_ODBB.csv'),\n",
    "         ('../data/raw/Dorje4014_ODBA3D_c.csv', '../data/raw/Dorje4014_ODBB.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_file_tuple(file_tuple):\n",
    "    dfraw1 = pd.read_csv(file_tuple[0], parse_dates=[['Day', 'Time']], na_values=[' '])\n",
    "    dfraw1.set_index('EventID', inplace=True)\n",
    "    print('%d rows read from %s' % (len(dfraw1), file_tuple[0]))\n",
    "    dup = dfraw1.index.duplicated(keep='first')\n",
    "    dfraw1 = dfraw1[dup==False]\n",
    "    print('%d duplicate indices removed, %d records remaining' % (dup.sum(), len(dfraw1)))\n",
    "    dfraw2 = pd.read_csv(file_tuple[1], parse_dates=[['Day', 'Time']], na_values=[' '])\n",
    "    dfraw2.set_index('EventID', inplace=True)\n",
    "    print('%d rows read from %s' % (len(dfraw2), file_tuple[1]))\n",
    "    dup = dfraw2.index.duplicated(keep='first')\n",
    "    dfraw2 = dfraw2[dup==False]\n",
    "    print('%d duplicate indices removed, %d records remaining' % (dup.sum(), len(dfraw2)))\n",
    "    dfraw = dfraw1.join(dfraw2, how='inner', rsuffix='_dup')\n",
    "    print('Inner-joined dataframes on index, %d records remaining' % len(dfraw))\n",
    "    df = pd.DataFrame(dfraw.index)\n",
    "    df.set_index('EventID', inplace=True)\n",
    "    df['dt'] = dfraw['Day_Time']\n",
    "    df['odba'] = dfraw['odba']\n",
    "    acc_x = data=dfraw[[col for col in dfraw.columns if col.startswith('ACCX')]].values\n",
    "    acc_y = data=dfraw[[col for col in dfraw.columns if col.startswith('ACCY')]].values\n",
    "    acc_z = data=dfraw[[col for col in dfraw.columns if col.startswith('ACCZ')]].values\n",
    "    df['groundspeed'] = dfraw['Groundspeed']\n",
    "    df['pressuredelta'] = dfraw['PressureVerticalVelocity']\n",
    "    df['vvelocity'] = dfraw['VVelocity']\n",
    "    df['uvelocity'] = dfraw['UVelocity']\n",
    "    altitude = dfraw.HeightMSL-dfraw.SRTMElevation  # height above MSL - SRTM elevation = actual altitude?\n",
    "    altitude[altitude < 0] = 0.0        # normalising erroneous altitudes \n",
    "    altitude[altitude > 1000] = 1000.0  # (too high or too low)\n",
    "    df['altitude'] = altitude\n",
    "    temperature = dfraw['Temperature']-273.15  # in Celsius\n",
    "    temperature[temperature < -20] = -20.0 # normalising erroneous temperatures\n",
    "    temperature[temperature > 50] = 50.0 # normalising erroneous temperatures\n",
    "    df['temperature'] = temperature\n",
    "    df['humidity'] = dfraw['RelativeHumidity']\n",
    "    try:\n",
    "        df['state'] = dfraw['Behavior']\n",
    "    except KeyError:\n",
    "        df['state'] = dfraw['Behaviour']\n",
    "    \n",
    "    # processing acceleration data\n",
    "    # purging records that do not contain 40 acceleration values on each axis\n",
    "    invalid = np.apply_along_axis(lambda x: np.any(np.isnan(x)), 1, acc_x) \\\n",
    "              | np.apply_along_axis(lambda x: np.any(np.isnan(x)), 1, acc_y) \\\n",
    "              | np.apply_along_axis(lambda x: np.any(np.isnan(x)), 1, acc_z)\n",
    "    if invalid.any():\n",
    "        df = df[invalid==False]\n",
    "        acc_x = acc_x[invalid==False]\n",
    "        acc_y = acc_y[invalid==False]\n",
    "        acc_z = acc_z[invalid==False]\n",
    "        print('Purged %d records with less than 40 acc values on at least 1 axis, %d remaining' % (invalid.sum(), len(df)))\n",
    "    # standardising all series to mean 0 and std 1\n",
    "    acc_x = (acc_x-(acc_x.mean()))/(acc_x.std())\n",
    "    acc_y = (acc_y-(acc_y.mean()))/(acc_y.std())\n",
    "    acc_z = (acc_z-(acc_z.mean()))/(acc_z.std())\n",
    "    \n",
    "    # creating composite measures for each acceleration burst: mean and std\n",
    "    df['acc_x_mean'] = np.apply_along_axis(lambda x: x.mean(), 1, acc_x)\n",
    "    df['acc_x_std'] = np.apply_along_axis(lambda x: x.std(), 1, acc_x)\n",
    "    df['acc_y_mean'] = np.apply_along_axis(lambda x: x.mean(), 1, acc_y)\n",
    "    df['acc_y_std'] = np.apply_along_axis(lambda x: x.std(), 1, acc_y)\n",
    "    df['acc_z_mean'] = np.apply_along_axis(lambda x: x.mean(), 1, acc_z)\n",
    "    df['acc_z_std'] = np.apply_along_axis(lambda x: x.std(), 1, acc_z)\n",
    "    \n",
    "    # dropping records with NaN values\n",
    "    df_ = df.dropna(axis=0, how='any')\n",
    "    if len(df_) != len(df):\n",
    "        print('Purged %d records containing NaN values, %d remaining' % (len(df)-len(df_), len(df_)))\n",
    "    \n",
    "    return {'name': '/'.join(dfraw.Individual.dropna().unique()), 'data': df_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16293 rows read from ../data/raw/Baen4185_ODBA3D_c.csv\n",
      "3 duplicate indices removed, 16290 records remaining\n",
      "16279 rows read from ../data/raw/Baen4185_ODBB.csv\n",
      "3 duplicate indices removed, 16276 records remaining\n",
      "Inner-joined dataframes on index, 16276 records remaining\n",
      "Purged 423 records containing NaN values, 15853 remaining\n",
      "\n",
      "9172 rows read from ../data/raw/Dorje4014_ODBA3D_c.csv\n",
      "0 duplicate indices removed, 9172 records remaining\n",
      "16486 rows read from ../data/raw/Dorje4014_ODBB.csv\n",
      "0 duplicate indices removed, 16486 records remaining\n",
      "Inner-joined dataframes on index, 9171 records remaining\n",
      "Purged 2358 records with less than 40 acc values on at least 1 axis, 6813 remaining\n",
      "Purged 2544 records containing NaN values, 4269 remaining\n"
     ]
    }
   ],
   "source": [
    "indiv1 = process_file_tuple(files[0])\n",
    "print('')\n",
    "indiv2 = process_file_tuple(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rest            10392\n",
       "Fly              3076\n",
       "Restless         1835\n",
       "WHR               308\n",
       "Others            131\n",
       "Flap-flight        39\n",
       "Feed               27\n",
       "Flap-land          23\n",
       "Flap-takeoff       22\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indiv1['data']['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rest          2719\n",
       "Fly           1081\n",
       "Restless       389\n",
       "Others          45\n",
       "Flap_fligh      16\n",
       "WHR             13\n",
       "Feed             3\n",
       "Flap_land        2\n",
       "Flap_takeo       1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indiv2['data']['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 15853 records to data_clean_Thang_Kaar_Baen_4185.csv\n",
      "Exported 4269 records to data_clean_Thang_Kaar_Dorje_4014.csv\n"
     ]
    }
   ],
   "source": [
    "for indiv in [indiv1, indiv2]:\n",
    "    file_name = 'data_clean_' + indiv['name'].replace(' ', '_').replace('(','').replace(')','') + '.csv'\n",
    "    indiv['data'].to_csv('../data/processed/%s' % file_name, header=True, index=True)\n",
    "    print('Exported %d records to %s' % (len(indiv['data']), file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
